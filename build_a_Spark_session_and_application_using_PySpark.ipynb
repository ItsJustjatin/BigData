{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AIM**-To create and build a Spark session and application using PySpark, perform operations on a sample dataset, and verify the setup."
      ],
      "metadata": {
        "id": "bClpmn1l17rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEPS**:"
      ],
      "metadata": {
        "id": "sb401PLy2tz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install and Setup Dependencies**\n",
        "\n",
        "1.Install Java:\n",
        "\n",
        "Apache Spark requires Java to run. The following command installs Java 8\n",
        "\n",
        "2.Download Apache Spark:\n",
        "\n",
        "Spark version 3.0.0 compatible with Hadoop 2.7 is downloaded\n",
        "\n",
        "The downloaded .tgz file is extracted\n",
        "\n",
        "3.Install Findspark and PySpark:\n",
        "\n",
        "Findspark: Makes it easier for Python to locate Spark installations.\n",
        "\n",
        "PySpark: Python bindings for Apache Spark, version 3.0.0 is installed to match the downloaded Spark version.\n"
      ],
      "metadata": {
        "id": "2Egnyn_C3FZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null  # Suppress output\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!pip install findspark\n",
        "!pip install pyspark==3.0.0  # Install PySpark matching Spark version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZHovhp90cdl",
        "outputId": "af0e0c90-f730-428a-8ec6-2be9e7487886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pyspark==3.0.0 in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.0.0) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Configure Environment Variables**\n",
        "\n",
        "1.Set Java and Spark Home Paths:\n",
        "\n",
        "Define the paths for Java and Spark installations\n",
        "\n",
        "2.Initialize Findspark:\n",
        "\n",
        "Findspark is initialized to link the Spark environment\n"
      ],
      "metadata": {
        "id": "e_F7ejZO3q1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "DgQ31Ap60d9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "g6fLKzKu0_Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Build a SparkSession**\n",
        "\n",
        "1. Create a Spark Session:\n",
        "\n",
        "A SparkSession is built explicitly with the following parameters:\n",
        "\n",
        "master(\"local[*]\"): Indicates that the application will run locally, utilizing all available CPU cores.\n",
        "\n",
        "appName(\"MySparkApp\"): Names the Spark application as \"MySparkApp\".\n",
        "\n",
        "config: Additional configurations (e.g., specifying the classpath for driver dependencies).\n"
      ],
      "metadata": {
        "id": "HoiUi84i4Cdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build SparkSession explicitly with configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.driver.extraClassPath\", os.path.join(os.environ[\"SPARK_HOME\"], \"jars/*\")) \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "x9CYjcww1CpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify Spark Session Creation**"
      ],
      "metadata": {
        "id": "bFS9pGDT4SVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify SparkSession creation\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykgzEKtW1FkY",
        "outputId": "bce1f58a-4967-4825-b542-d6fcdee480b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x793fa83425f0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Define and Load a Dataset**\n",
        "\n",
        "1.Define a Sample Dataset:\n",
        "\n",
        "A Python list of dictionaries is created to represent the dataset"
      ],
      "metadata": {
        "id": "YxMmxJqN4d5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample dataset\n",
        "data = [\n",
        "    {\"Name\": \"Alice\", \"Age\": 25, \"City\": \"New York\"},\n",
        "    {\"Name\": \"Bob\", \"Age\": 30, \"City\": \"San Francisco\"},\n",
        "    {\"Name\": \"Cathy\", \"Age\": 27, \"City\": \"Seattle\"},\n",
        "    {\"Name\": \"David\", \"Age\": 35, \"City\": \"Austin\"}\n",
        "]"
      ],
      "metadata": {
        "id": "uxL5yqAl1gxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Define a Schema:\n",
        "\n",
        "A StructType schema is created to define the structure of the DataFrame"
      ],
      "metadata": {
        "id": "x_hOfPMo4l9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define a schema for the dataset\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"City\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "N77Pdgds1icl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Create a DataFrame:\n",
        "\n",
        "The dataset is converted into a Spark DataFrame using the defined schema"
      ],
      "metadata": {
        "id": "Tt5fICyf4s1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, schema=schema)"
      ],
      "metadata": {
        "id": "NpsFVqma1pMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Show the DataFrame:\n",
        "\n",
        "Display the contents of the DataFrame"
      ],
      "metadata": {
        "id": "P3kWra9_4yNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the DataFrame\n",
        "print(\"Dataset loaded into Spark DataFrame:\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMwKnb-X1sRy",
        "outputId": "ee07d158-7667-4190-f041-16ada6023c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded into Spark DataFrame:\n",
            "+-----+---+-------------+\n",
            "| Name|Age|         City|\n",
            "+-----+---+-------------+\n",
            "|Alice| 25|     New York|\n",
            "|  Bob| 30|San Francisco|\n",
            "|Cathy| 27|      Seattle|\n",
            "|David| 35|       Austin|\n",
            "+-----+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Perform Data Transformations**\n",
        "1.Filter Operation:"
      ],
      "metadata": {
        "id": "RaszuTa845s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a simple transformation\n",
        "print(\"Filter rows where Age > 30:\")\n",
        "df_filtered = df.filter(df[\"Age\"] > 30)\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVD8kF3r1wpy",
        "outputId": "1e22bda4-f10e-4ec9-cdd7-384180f0023d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filter rows where Age > 30:\n",
            "+-----+---+------+\n",
            "| Name|Age|  City|\n",
            "+-----+---+------+\n",
            "|David| 35|Austin|\n",
            "+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Group-By and Count Operation:"
      ],
      "metadata": {
        "id": "A8CgUyYV5A0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a group-by and count operation\n",
        "print(\"Group by City and count:\")\n",
        "df_grouped = df.groupBy(\"City\").count()\n",
        "df_grouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6md7a32r1yKH",
        "outputId": "e329a5a1-73b7-492f-d683-d99f1b21b158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Group by City and count:\n",
            "+-------------+-----+\n",
            "|         City|count|\n",
            "+-------------+-----+\n",
            "|San Francisco|    1|\n",
            "|       Austin|    1|\n",
            "|      Seattle|    1|\n",
            "|     New York|    1|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULT**:\n",
        "The code successfully demonstrates:\n",
        "\n",
        "1. Installing and configuring Spark and Java.\n",
        "2. Building a SparkSession with explicit configurations.\n",
        "3. Creating a DataFrame from a Python dataset.\n",
        "4. Performing transformations like filtering and grouping."
      ],
      "metadata": {
        "id": "04Qr3RpC5F4S"
      }
    }
  ]
}