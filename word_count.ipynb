{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AIM**: The aim of the provided Spark code is to perform word count on a given text file. This is a fundamental example of distributed data processing, demonstrating how to use Apache Spark to count the frequency of each word in a dataset."
      ],
      "metadata": {
        "id": "HV9DcP30Djkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEPS:**"
      ],
      "metadata": {
        "id": "JzoGnkq-Dud6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toVUZ2Ee513u"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initialize SparkSession\n",
        "\n",
        "Purpose: Creates a SparkSession, which is the entry point for interacting with\n",
        "Spark functionality.\n",
        "\n",
        "appName: Specifies the name of the application (\"WordCount\")."
      ],
      "metadata": {
        "id": "-Ky0ct11DzIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()"
      ],
      "metadata": {
        "id": "ZiY1zGK96zX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load the Text File\n",
        "\n",
        "\n",
        "*   Purpose: Reads the input text file into an RDD (Resilient Distributed Dataset).\n",
        "*   RDD: A distributed collection of elements that can be processed in parallel across the cluster.\n",
        "*   textFile: Splits the file into lines and stores them in the RDD."
      ],
      "metadata": {
        "id": "qp_wbq9ID-hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text file into an RDD\n",
        "input_file = \"/content/hi.txt\"  # Replace with your file path\n",
        "text_rdd = spark.sparkContext.textFile(input_file)"
      ],
      "metadata": {
        "id": "N_1ZsTP761u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Split Lines into Words\n",
        "\n",
        "\n",
        "*   Purpose: Breaks each line of text into individual words.\n",
        "*   flatMap: Transforms each line into multiple words. It flattens the results into a single RDD of words.\n",
        "\n",
        "4. Map Words to Key-Value Pairs\n",
        "\n",
        "\n",
        "*   Purpose: Maps each word to a tuple (word, 1).\n",
        "*   Key-Value Pair: Enables grouping by word to perform aggregation later.\n",
        "\n",
        "5. Count Word Frequencies\n",
        "\n",
        "\n",
        "*   Purpose: Aggregates the values (counts) for each unique key (word).\n",
        "*   reduceByKey: Combines the counts for each word by summing the 1s.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oK4xvVFVEmmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations to perform word count\n",
        "word_counts = (\n",
        "    text_rdd.flatMap(lambda line: line.split())  # Split lines into words\n",
        "    .map(lambda word: (word, 1))  # Map each word to (word, 1)\n",
        "    .reduceByKey(lambda a, b: a + b)  # Reduce by key to count occurrences\n",
        ")"
      ],
      "metadata": {
        "id": "fzxW2Amv64RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Collect Results\n",
        "\n",
        "*   Purpose: Gathers the results from the distributed RDD to the driver node.\n",
        "*   collect: Converts the distributed data into a local Python list.\n",
        "*   Print Loop: Iterates through the list and prints each word and its count.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rp8PChlwFZsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect and print results\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USScBk8U67M0",
        "outputId": "cf4fb385-b1c9-4055-bd95-0a69c247c198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And: 2\n",
            "we: 4\n",
            "want,: 1\n",
            "is: 2\n",
            "feel: 1\n",
            "like: 3\n",
            "are: 2\n",
            "beggars.: 1\n",
            "when: 2\n",
            "i: 1\n",
            "use: 1\n",
            "serve: 1\n",
            "curd: 1\n",
            "sundays),: 1\n",
            "only: 2\n",
            "curd,: 1\n",
            "give: 3\n",
            "accordingly.: 1\n",
            "stand: 1\n",
            "cauliflower: 1\n",
            "anything: 1\n",
            "wish.: 1\n",
            "Do: 1\n",
            "have: 1\n",
            "question: 1\n",
            "too?: 1\n",
            "Then: 1\n",
            "queue: 1\n",
            "there,: 1\n",
            "waiting: 1\n",
            "behind,: 1\n",
            "attitude.: 1\n",
            "attitude: 1\n",
            "for: 2\n",
            "selecting: 1\n",
            "the: 4\n",
            "chicken,: 1\n",
            "its: 2\n",
            "our: 1\n",
            "choice: 1\n",
            "which: 1\n",
            "piece: 2\n",
            "asking: 1\n",
            "a: 1\n",
            "better: 1\n",
            "made: 1\n",
            "to: 4\n",
            "some: 3\n",
            "Personally: 1\n",
            "(on: 1\n",
            "people: 3\n",
            "prefer: 1\n",
            "need: 1\n",
            "onion: 1\n",
            "and: 3\n",
            "so: 1\n",
            "on.: 1\n",
            "I: 1\n",
            "it: 2\n",
            "fried: 1\n",
            "rice,: 1\n",
            "pick: 1\n",
            "according: 1\n",
            "their: 1\n",
            "also: 1\n",
            "how: 1\n",
            "you: 1\n",
            "guys: 1\n",
            "statements: 1\n",
            "with: 2\n",
            "So: 1\n",
            "want: 2\n",
            "keep: 1\n",
            "your: 1\n",
            "yourself.: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Stop the SparkSession"
      ],
      "metadata": {
        "id": "pTpnrsPAFr_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "tTSRrbMJ69nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULT**: This code efficiently processes large datasets by distributing computation across the cluster, showcasing the power of Spark's parallelism."
      ],
      "metadata": {
        "id": "7dlLnkx2F2B2"
      }
    }
  ]
}